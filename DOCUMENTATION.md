# Kloufi-Scrape Documentation

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Quick Start](#quick-start)
4. [Configuration](#configuration)
5. [Local Testing](#local-testing)
6. [Production Deployment](#production-deployment)
7. [Docker Deployment](#docker-deployment)
8. [Monitoring & Alerts](#monitoring--alerts)
9. [Scraper Development](#scraper-development)
10. [Troubleshooting](#troubleshooting)

---

## ğŸ“Œ Project Overview

Kloufi-Scrape is a production-ready web scraping system designed for continuous, automated data collection from Algerian websites across 5 categories:

| Category | Description | Sites |
|----------|-------------|-------|
| **immobilier** | Real Estate | OuedKniss, Krello, Lkeria, Beytic, etc. |
| **voiture** | Vehicles | OuedKniss, Tonobiles, AutoBessah, etc. |
| **emploi** | Jobs | Emploitic, AlgerieJob, CVYA, etc. |
| **electromenager** | Home Appliances | Jumia, Starmania, WebStar, etc. |
| **multimedia** | Electronics | Jumia, Informatics, etc. |

### Key Features

- âœ… **Continuous Auto-Scraping** - Runs 24/7 until manually stopped
- âœ… **Smart Proxy Rotation** - Avoids blocks with intelligent proxy management
- âœ… **Dual Storage** - Elasticsearch (production) & JSON (testing)
- âœ… **Real-time Alerts** - Telegram/Email notifications for issues
- âœ… **Docker Support** - Easy deployment with docker-compose
- âœ… **Graceful Shutdown** - Clean stop with data preservation

---

## ğŸ— Architecture

```
kloufi-scrape/
â”œâ”€â”€ config/                    # Centralized configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py           # All config in one place
â”‚
â”œâ”€â”€ core/                      # Core orchestration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dispatcher.py         # Main scraping orchestrator
â”‚   â”œâ”€â”€ category_runner.py    # Runs all sites for a category
â”‚   â”œâ”€â”€ alerting.py           # Telegram/Email alerts
â”‚   â””â”€â”€ storage.py            # Unified data storage
â”‚
â”œâ”€â”€ scraper/                   # Scraping infrastructure
â”‚   â”œâ”€â”€ main.py               # Legacy entry point
â”‚   â”œâ”€â”€ browser/              # Browser fingerprinting
â”‚   â”‚   â”œâ”€â”€ fingerprint.py
â”‚   â”‚   â”œâ”€â”€ stealth.py
â”‚   â”‚   â””â”€â”€ user_agents.py
â”‚   â”œâ”€â”€ crawler/              # Crawl4AI integration
â”‚   â”‚   â”œâ”€â”€ crawler_runner.py
â”‚   â”‚   â””â”€â”€ fallback_proxyium.py
â”‚   â”œâ”€â”€ detection/            # Block/captcha detection
â”‚   â”‚   â”œâ”€â”€ block_detector.py
â”‚   â”‚   â””â”€â”€ captcha_detector.py
â”‚   â”œâ”€â”€ proxy/                # Proxy management
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”‚   â”œâ”€â”€ proxy_scoring.py
â”‚   â”‚   â””â”€â”€ proxy_sources.py
â”‚   â””â”€â”€ utils/                # Utilities
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ storage.py
â”‚
â”œâ”€â”€ sites/                     # Site-specific scrapers
â”‚   â”œâ”€â”€ immobilier/
â”‚   â”‚   â”œâ”€â”€ ouedkniss/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py       # Listing scraper
â”‚   â”‚   â”‚   â””â”€â”€ scrape_details.py
â”‚   â”‚   â”œâ”€â”€ krello/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ voiture/
â”‚   â”œâ”€â”€ emploi/
â”‚   â”œâ”€â”€ electromenager/
â”‚   â””â”€â”€ multimedia/
â”‚
â”œâ”€â”€ utils/                     # Category-specific utilities
â”‚   â”œâ”€â”€ immobilier.py         # Real estate normalization
â”‚   â”œâ”€â”€ voiture.py            # Vehicle normalization
â”‚   â”œâ”€â”€ emploi.py             # Job normalization
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ insert2db/                 # Elasticsearch integration
â”‚   â””â”€â”€ insert_scrape.py
â”‚
â”œâ”€â”€ scripts/                   # Deployment & testing scripts
â”‚   â”œâ”€â”€ deploy.sh             # Production deployment
â”‚   â””â”€â”€ local_test.py         # Local testing runner
â”‚
â”œâ”€â”€ docker/                    # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/                      # Scraped data (production)
â”œâ”€â”€ logs/                      # Log files
â”œâ”€â”€ junk_test/                 # Local test output (gitignored)
â”‚
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ DOCUMENTATION.md          # This file
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DISPATCHER                               â”‚
â”‚                    (core/dispatcher.py)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼               â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚immobilierâ”‚    â”‚ voiture â”‚    â”‚ emploi  â”‚    â”‚   ...   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”‚
    â”‚Category â”‚    â”‚Category â”‚    â”‚Category â”‚        â”‚
    â”‚ Runner  â”‚    â”‚ Runner  â”‚    â”‚ Runner  â”‚        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
         â”‚              â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”       â”‚              â”‚
   â”‚ OuedKniss â”‚  â”‚ Tonobiles â”‚       â”‚              â”‚
   â”‚  Krello   â”‚  â”‚  CardiaS  â”‚       â”‚              â”‚
   â”‚   ...     â”‚  â”‚    ...    â”‚       â”‚              â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚              â”‚
         â”‚              â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚ STORAGE â”‚
                    â”‚(core/)  â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  JSON   â”‚ (Local Testing)   â”‚Elastic- â”‚ (Production)
    â”‚  Files  â”‚                   â”‚ search  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose (for containerized deployment)
- Elasticsearch 8.x (for production data storage)

### Installation

```bash
# Clone repository
git clone <your-repo-url>
cd kloufi-scrape

# Create virtual environment
python -m venv .venv

# Activate (Linux/Mac)
source .venv/bin/activate

# Activate (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Setup Crawl4AI browser
crawl4ai-setup

# Copy environment template
cp .env.example .env
# Edit .env with your settings
```

### Run Locally (Testing)

```bash
# Set local environment
export KLOUFI_ENV=local  # Linux/Mac
$env:KLOUFI_ENV="local"  # Windows PowerShell

# Run with local testing script
python scripts/local_test.py --category immobilier

# Or run dispatcher directly
python core/dispatcher.py --single-run --categories immobilier
```

### Run in Production

```bash
# Set production environment
export KLOUFI_ENV=production

# Run dispatcher (continuous mode)
python core/dispatcher.py
```

---

## âš™ï¸ Configuration

All configuration is centralized in `config/settings.py`. Settings can be overridden via environment variables.

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `KLOUFI_ENV` | `local` | Environment: `local`, `production`, `docker` |
| `ELASTICSEARCH_HOST` | `http://localhost:9200` | Elasticsearch URL |
| `ELASTICSEARCH_USERNAME` | `elastic` | ES username |
| `ELASTICSEARCH_PASSWORD` | - | ES password |
| `REDIS_HOST` | `localhost` | Redis host |
| `REDIS_PORT` | `6379` | Redis port |
| `TELEGRAM_BOT_TOKEN` | - | Telegram bot token for alerts |
| `TELEGRAM_CHAT_ID` | - | Telegram chat ID for alerts |
| `CONTINUOUS_MODE` | `true` | Run continuously or single-run |
| `CYCLE_DELAY` | `3600` | Seconds between scrape cycles |
| `MAX_CONCURRENT_LISTING` | `2` | Concurrent listing page scrapers |
| `MAX_CONCURRENT_DETAILS` | `15` | Concurrent detail page scrapers |

### Environment Modes

| Mode | Data Storage | Concurrency | Use Case |
|------|-------------|-------------|----------|
| `local` | JSON in `junk_test/` | Low (1-3) | Development & testing |
| `production` | Elasticsearch | High (10-15) | Live deployment |
| `docker` | Elasticsearch | High (10-15) | Containerized deployment |

---

## ğŸ§ª Local Testing

Local testing mode saves data to `junk_test/` (which is gitignored) instead of Elasticsearch.

### Run Tests

```bash
# Test single category
python scripts/local_test.py --category immobilier

# Test specific site (not yet implemented in runner)
python scripts/local_test.py --category voiture

# Test with continuous mode (keep running)
python scripts/local_test.py --category emploi --continuous
```

### View Test Output

```bash
# List scraped files
ls -la junk_test/immobilier/

# View a scraped item
cat junk_test/immobilier/ouedkniss/*.json | head -100
```

### Test Configuration

Local mode automatically:
- Reduces concurrency (fewer parallel requests)
- Saves to JSON files instead of Elasticsearch
- Enables more verbose logging

---

## ğŸ–¥ Production Deployment

### Option 1: Systemd Service (Recommended)

```bash
# Run deployment script
sudo ./scripts/deploy.sh

# Configure
sudo nano /opt/kloufi-scrape/.env

# Start service
sudo systemctl start kloufi-scraper

# Enable on boot
sudo systemctl enable kloufi-scraper

# View logs
sudo journalctl -u kloufi-scraper -f
```

### Option 2: Manual Setup

```bash
# Create directory
sudo mkdir -p /opt/kloufi-scrape
sudo cp -r . /opt/kloufi-scrape/

# Setup virtual environment
cd /opt/kloufi-scrape
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
crawl4ai-setup

# Configure
cp .env.example .env
nano .env  # Add your settings

# Run with screen/tmux
screen -S kloufi
export KLOUFI_ENV=production
python core/dispatcher.py
# Ctrl+A, D to detach
```

### Elasticsearch Setup

```bash
# Install Elasticsearch
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.0-amd64.deb
sudo dpkg -i elasticsearch-8.11.0-amd64.deb

# Configure
sudo nano /etc/elasticsearch/elasticsearch.yml
# Set: network.host: 0.0.0.0
# Set: discovery.type: single-node

# Start
sudo systemctl start elasticsearch
sudo systemctl enable elasticsearch

# Get password
sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
```

---

## ğŸ³ Docker Deployment

### Quick Start

```bash
cd docker

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f scraper

# Stop
docker-compose down
```

### Scrape Specific Categories

```bash
# Only immobilier and voiture
CATEGORIES="immobilier voiture" docker-compose up -d scraper
```

### With Monitoring Stack

```bash
# Start with Kibana, Prometheus, Grafana
docker-compose --profile monitoring up -d
```

### Services

| Service | Port | Description |
|---------|------|-------------|
| scraper | - | Main scraper container |
| redis | 6379 | Proxy scoring persistence |
| elasticsearch | 9200 | Data storage |
| kibana | 5601 | Data visualization (optional) |
| prometheus | 9090 | Metrics (optional) |
| grafana | 3000 | Dashboards (optional) |

---

## ğŸ“¢ Monitoring & Alerts

### Telegram Alerts

1. Create a bot via [@BotFather](https://t.me/botfather)
2. Get your chat ID from [@userinfobot](https://t.me/userinfobot)
3. Add to `.env`:
   ```
   TELEGRAM_BOT_TOKEN=your_bot_token
   TELEGRAM_CHAT_ID=your_chat_id
   ```

### Alert Types

| Alert | Level | Trigger |
|-------|-------|---------|
| Startup | â„¹ï¸ INFO | Scraper started |
| Progress | â„¹ï¸ INFO | Every 100 items scraped |
| Category Complete | âœ… SUCCESS | Category finished |
| Cycle Complete | âœ… SUCCESS | Full cycle finished |
| High Errors | ğŸš¨ ERROR | 10+ consecutive errors |
| Captcha Flood | âš ï¸ WARNING | 3+ captchas detected |
| IP Block | âš ï¸ WARNING | 5+ blocks detected |
| Shutdown | â„¹ï¸ INFO | Scraper stopped |

### Health Check

```python
from core.alerting import get_alert_manager

manager = get_alert_manager()
health = await manager.health_check()
print(health)
```

---

## ğŸ‘¨â€ğŸ’» Scraper Development

### Adding a New Site

1. Create site directory:
   ```bash
   mkdir -p sites/immobilier/newsite
   ```

2. Create `main.py`:
   ```python
   # sites/immobilier/newsite/main.py
   import asyncio
   from core.storage import get_storage
   from scraper.proxy.proxy_manager import ProxyManager
   
   async def run_scraper(
       proxy_manager: ProxyManager = None,
       config = None,
       shutdown_event: asyncio.Event = None,
   ):
       storage = get_storage("immobilier", "newsite")
       items_scraped = 0
       errors = 0
       
       # Your scraping logic here
       # Use storage.save(data) to store items
       
       return {
           "items_scraped": items_scraped,
           "errors": errors,
       }
   ```

3. The dispatcher will auto-discover it on next run.

### Site Structure

```
sites/{category}/{site}/
â”œâ”€â”€ main.py           # Entry point with run_scraper()
â”œâ”€â”€ scrape_details.py # Detail page scraper
â””â”€â”€ __pycache__/
```

### Using Crawl4AI

```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from scraper.browser.fingerprint import build_context

config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    page_timeout=60000,
    wait_until="domcontentloaded",
    js_code=["window.scrollTo(0, document.body.scrollHeight);"],
    delay_before_return_html=3,
)

async with AsyncWebCrawler(
    proxy=proxy,
    browser_context=build_context(),
    headless=True,
) as crawler:
    result = await crawler.arun(url=url, config=config)
    if result.success:
        html = result.html
        # Parse with BeautifulSoup
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### Browser Not Found
```bash
# Reinstall browsers
crawl4ai-setup
playwright install chromium --with-deps
```

#### Elasticsearch Connection Failed
```bash
# Check if running
curl -u elastic:password http://localhost:9200

# Check logs
sudo journalctl -u elasticsearch -f
```

#### High Memory Usage
```bash
# Reduce concurrency in .env
MAX_CONCURRENT_DETAILS=5
MAX_CONCURRENT_LISTING=1
```

#### Proxy Errors
```bash
# Clear proxy scores
rm data/proxy_scores.json
# Restart scraper
```

#### Captcha Flood
- Reduce concurrency
- Add longer delays between requests
- Consider using premium proxies

### Log Files

| Log | Location | Description |
|-----|----------|-------------|
| Main | `logs/scraper.log` | All scraping activity |
| Service | `logs/service.log` | Systemd service output |
| Errors | `logs/service-error.log` | Service errors |

### Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
python core/dispatcher.py --single-run
```

---

## ğŸ“„ License

[Your License Here]

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch
3. Run local tests
4. Submit a pull request

---

*Last updated: February 2026*
