version: '3.8'

# ============================================================================
# KLOUFI-SCRAPE DOCKER COMPOSE
# ============================================================================
# Production-ready Docker setup for continuous auto-scraping
#
# Usage:
#   # Start everything (continuous scraping)
#   docker-compose up -d
#
#   # View logs
#   docker-compose logs -f scraper
#
#   # Stop
#   docker-compose down
#
#   # Scrape specific category
#   CATEGORIES="immobilier voiture" docker-compose up -d scraper
# ============================================================================

x-common-env: &common-env
  KLOUFI_ENV: docker
  PYTHONUNBUFFERED: 1
  # Elasticsearch
  ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:-http://elasticsearch:9200}
  ELASTICSEARCH_USERNAME: ${ELASTICSEARCH_USERNAME:-elastic}
  ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-changeme}
  # Redis
  REDIS_HOST: ${REDIS_HOST:-redis}
  REDIS_PORT: ${REDIS_PORT:-6379}
  # Alerting (optional)
  TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
  TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
  # Scheduling
  CONTINUOUS_MODE: ${CONTINUOUS_MODE:-true}
  CYCLE_DELAY: ${CYCLE_DELAY:-3600}
  # Scraper settings
  MAX_CONCURRENT_LISTING: ${MAX_CONCURRENT_LISTING:-2}
  MAX_CONCURRENT_DETAILS: ${MAX_CONCURRENT_DETAILS:-15}

services:
  # ==========================================================================
  # MAIN SCRAPER SERVICE
  # ==========================================================================
  scraper:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: kloufi-scraper
    environment:
      <<: *common-env
      CATEGORIES: ${CATEGORIES:-}
    volumes:
      - ../data:/app/data
      - ../logs:/app/logs
      - /dev/shm:/dev/shm  # Chromium performance
    depends_on:
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import sys; sys.exit(0)'"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    networks:
      - kloufi-network
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ==========================================================================
  # REDIS - Proxy scoring & state persistence
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: kloufi-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - kloufi-network

  # ==========================================================================
  # ELASTICSEARCH - Data storage
  # ==========================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: kloufi-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -s -u elastic:${ELASTICSEARCH_PASSWORD:-changeme} http://localhost:9200/_cluster/health | grep -q 'green\\|yellow'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - kloufi-network

  # ==========================================================================
  # KIBANA - Data visualization (optional)
  # ==========================================================================
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kloufi-kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD:-changeme}
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped
    profiles:
      - monitoring
    networks:
      - kloufi-network

  # ==========================================================================
  # PROMETHEUS - Metrics (optional)
  # ==========================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: kloufi-prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    restart: unless-stopped
    profiles:
      - monitoring
    networks:
      - kloufi-network

  # ==========================================================================
  # GRAFANA - Dashboards (optional)
  # ==========================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: kloufi-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    restart: unless-stopped
    profiles:
      - monitoring
    networks:
      - kloufi-network

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  redis-data:
    driver: local
  elasticsearch-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  kloufi-network:
    driver: bridge
#
# 4. Control concurrency:
#    MAX_WORKERS=50 MAX_CONCURRENT_DETAILS=30 docker-compose up -d
#
# 5. Run category-specific services:
#    docker-compose --profile category-specific up -d
#
# 6. View logs:
#    docker-compose logs -f scraper
#
# 7. Stop everything:
#    docker-compose down
#
# 8. Rebuild after code changes:
#    docker-compose up --build -d
